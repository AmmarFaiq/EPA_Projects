{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Twarc\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from twarc import Twarc\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for saving json, csv and formatted txt files\n",
    "def save_json(variable, filename):\n",
    "    with io.open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(unicode(json.dumps(variable, indent=4, ensure_ascii=False)))\n",
    "\n",
    "def save_csv(data, filename):\n",
    "    with io.open(filename, \"w\", encoding=\"utf-8\") as handle:\n",
    "        handle.write(u\"Source,Target,Weight\\n\")\n",
    "        for source, targets in sorted(data.items()):\n",
    "            for target, count in sorted(targets.items()):\n",
    "                if source != target and source is not None and target is not None:\n",
    "                      handle.write(source + u\",\" + target + u\",\" + unicode(count) + u\"\\n\")\n",
    "\n",
    "def save_text(data, filename):\n",
    "    with io.open(filename, \"w\", encoding=\"utf-8\") as handle:\n",
    "        for item, count in data.most_common():\n",
    "            handle.write(unicode(count) + \"\\t\" + item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the screen_name of the user retweeted, or None\n",
    "def retweeted_user(status):\n",
    "    if \"retweeted_status\" in status:\n",
    "        orig_tweet = status[\"retweeted_status\"]\n",
    "        if \"user\" in orig_tweet and orig_tweet[\"user\"] is not None:\n",
    "            user = orig_tweet[\"user\"]\n",
    "            if \"screen_name\" in user and user[\"screen_name\"] is not None:\n",
    "                return user[\"screen_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of screen_names that the user interacted with in this Tweet\n",
    "def get_interactions(status):\n",
    "    interactions = []\n",
    "    if \"in_reply_to_screen_name\" in status:\n",
    "        replied_to = status[\"in_reply_to_screen_name\"]\n",
    "        if replied_to is not None and replied_to not in interactions:\n",
    "            interactions.append(replied_to)\n",
    "    if \"retweeted_status\" in status:\n",
    "        orig_tweet = status[\"retweeted_status\"]\n",
    "        if \"user\" in orig_tweet and orig_tweet[\"user\"] is not None:\n",
    "            user = orig_tweet[\"user\"]\n",
    "            if \"screen_name\" in user and user[\"screen_name\"] is not None:\n",
    "                if user[\"screen_name\"] not in interactions:\n",
    "                    interactions.append(user[\"screen_name\"])\n",
    "    if \"quoted_status\" in status:\n",
    "        orig_tweet = status[\"quoted_status\"]\n",
    "        if \"user\" in orig_tweet and orig_tweet[\"user\"] is not None:\n",
    "            user = orig_tweet[\"user\"]\n",
    "            if \"screen_name\" in user and user[\"screen_name\"] is not None:\n",
    "                if user[\"screen_name\"] not in interactions:\n",
    "                    interactions.append(user[\"screen_name\"])\n",
    "    if \"entities\" in status:\n",
    "        entities = status[\"entities\"]\n",
    "        if \"user_mentions\" in entities:\n",
    "              for item in entities[\"user_mentions\"]:\n",
    "                if item is not None and \"screen_name\" in item:\n",
    "                    mention = item['screen_name']\n",
    "                    if mention is not None and mention not in interactions:\n",
    "                        interactions.append(mention)\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of hashtags found in the tweet\n",
    "def get_hashtags(status):\n",
    "    hashtags = []\n",
    "    if \"entities\" in status:\n",
    "        entities = status[\"entities\"]\n",
    "        if \"hashtags\" in entities:\n",
    "            for item in entities[\"hashtags\"]:\n",
    "                if item is not None and \"text\" in item:\n",
    "                    hashtag = item['text']\n",
    "                    if hashtag is not None and hashtag not in hashtags:\n",
    "                        hashtags.append(hashtag)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of URLs found in the Tweet\n",
    "def get_urls(status):\n",
    "    urls = []\n",
    "    if \"entities\" in status:\n",
    "        entities = status[\"entities\"]\n",
    "        if \"urls\" in entities:\n",
    "            for item in entities[\"urls\"]:\n",
    "                if item is not None and \"expanded_url\" in item:\n",
    "                    url = item['expanded_url']\n",
    "                    if url is not None and url not in urls:\n",
    "                        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the URLs to any images found in the Tweet\n",
    "def get_image_urls(status):\n",
    "    urls = []\n",
    "    if \"entities\" in status:\n",
    "        entities = status[\"entities\"]\n",
    "        if \"media\" in entities:\n",
    "            for item in entities[\"media\"]:\n",
    "                if item is not None:\n",
    "                    if \"media_url\" in item:\n",
    "                        murl = item[\"media_url\"]\n",
    "                        if murl not in urls:\n",
    "                            urls.append(murl)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main starts here\n",
    "\n",
    "# Add your own API key values here\n",
    "consumer_key= '0YJQlQtmUf0Y6N7NVu5rA33fk'\n",
    "consumer_secret= 'bduF2iguZjQSkq6G17jsX0yax9ICOcLWNYUicR99b51GLzeNam'\n",
    "access_token= '1017116258-GUN3YAWrElxuVDDkeazQui0rhOsuR08UNgIRYg2'\n",
    "access_token_secret= 'My6igTJbpw1nHtNwid3P9qksNicScwLxTlGrkFcDuTK53'\n",
    "\n",
    "twarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Check that search terms were provided at the command line\n",
    "target_list = ['PvdA', 'Partij van de Arbeid', 'Lodewijk Asscher', 'Marleen Barth', 'Moorlag', 'Aboutaleb', \n",
    "             'Arib', 'VVD', 'Volkspartij voor Vrijheid en Democratie', 'Mark Rutte', 'Eric Wiebes', \n",
    "             'Klaas Dijkhoff', 'Halbe Zijlstra', 'PVV',  'Partij van de Vrijheid', 'Geert Wilders', \n",
    "             'Martin Bosma', 'DENK', 'Tunahan Kuzu','D66', 'Democraten 66', 'Kajsa Ollongren', \n",
    "             'Alexander Pechtold', 'Jan Paternotte', 'Ingrid van Engelshoven', 'Wouter Koolmees', \n",
    "             'Sigrid Kaag', 'Stientje van Veldhoven', 'FvD', 'Forum voor Democratie', 'Forum', \n",
    "             'Thierry Baudet', 'Theo Hiddema', 'Yernaz Ramautarsing', 'Annabel Nanninga', 'CDA', \n",
    "             'Christendemocraten', 'Christendemocratie', 'Buma', 'Sybrand van Haersma-Buma', 'Mona Keizer', \n",
    "             'Wopke Hoekstra', 'Hugo de Jonge', 'Ferdinand Grapperhaus', 'GroenLinks', 'Jesse Klaver',\n",
    "             'ChristenUnie', 'Arie Slob', 'Carola Schouten','SGP', 'Kees van der Staaij', 'Partij voor de Dieren', \n",
    "             'Marianne Thieme', 'SP','Socialistische Partij', 'Lillian Marijnissen', 'Arjan Vliegenthart', \n",
    "             'Leefbaar Rotterdam', 'Joost Eerdmans', 'NIDA', 'Cemil Yildaz', 'Femke Halsema', '50plus', 'Henk Krol', \n",
    "             'NIDA', 'Cemil Yildaz', 'Groep de Mos', 'Richard de Mos', 'Rachid Guernaoui', 'Janice Roopram', \n",
    "             'Haagse stadspartij', 'Peter Bos', 'Joris Wijsmuller', 'Fatima Faid','Islam Democraten', 'Hasan Küçük', \n",
    "             'Arnoud van Doorn', 'Boudewijn Revis', 'Judith Oudshoorn-van Ginderen', 'Robert van Asten', \n",
    "             'Hanneke van der Werf', 'Dennis Groenewold', 'Arjen Kapteijns', 'Erlijn Wenink', 'Karen Gerbrands', \n",
    "             'Elias van Hees','Martijn Balster', 'Mikal Tseggai', 'Karsten Klein', 'Danielle Koster','Christine Teunissen', \n",
    "             'SP', 'Bart van Kent', 'Aisha Akhiat', 'Pieter Grinwis','#GR2018', '#Baudet', '#Pechtold', '#gr2018', \n",
    "             '#hijzeihetecht', '#forumvoorfantasie', '#GR18', '#FVD', '#D66', '#PVV', '#fvdemocratie', '#GR2018RT', \n",
    "             '#FvD', '#PvdA', '#d66', '#gemeenteraadsverkiezingen', '#GR2', '#gemeenteraadsverkiezingen2018', '#baudet', \n",
    "             '#VVD', '#fvd', '#hoedan', '#StemPVV', '#jinek', '#GroenLinks', '#gr2018RT', '#pvda','#partijkartel', \n",
    "             '#antisemitisme', '#buitenhof', '#Gemeenteraads', '#referendum', '#Gr2018', '#groenlinks', \n",
    "             '#pechtold', '#Jinek', '#Coentunnel', '#Asscher', '#CDA', '#GR20', '#LelystadAirport', \n",
    "             '#Buma', '#PvdAGroenLinks', '#leefbaarrotterdam', '#Ascher', '#demoniseren','#Kuzu', \n",
    "             '#lijsttrekkers', '#integratie', '#voltooidleven', '#campagne', '#GRV2018', '#Klaver', \n",
    "             '#nepnieuws', '#dwdd', '#asscher', '#klaver', '#cda', '#Gemeenteraadsverkiezingen', \n",
    "             '#klimaat', '#Marijnissen', '#ThierryBaudet', '#Rutte', '#hartvoordenhaag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(sys.argv) > 1):\n",
    "    target_list = sys.argv[1:]\n",
    "else:\n",
    "    print(\"No search terms provided. Exiting.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "num_targets = len(target_list)\n",
    "for count, target in enumerate(target_list):\n",
    "    print(str(count + 1) + \"/\" + str(num_targets) + \" searching on target: \" + target)\n",
    "\n",
    "# Create a separate save directory for each search query\n",
    "# Since search queries can be a whole sentence, we'll check the length\n",
    "# and simply number it if the query is overly long\n",
    "\n",
    "save_dir = ''\n",
    "if len(target) < 30:\n",
    "    save_dir = target.replace(\" \", \"_\")\n",
    "else:\n",
    "    save_dir = \"target_\" + str(count + 1)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    print(\"Creating directory: \" + save_dir)\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Variables for capturing stuff\n",
    "tweets_captured = 0\n",
    "influencer_frequency_dist = Counter()\n",
    "mentioned_frequency_dist = Counter()\n",
    "hashtag_frequency_dist = Counter()\n",
    "url_frequency_dist = Counter()\n",
    "user_user_graph = {}\n",
    "user_hashtag_graph = {}\n",
    "hashtag_hashtag_graph = {}\n",
    "all_image_urls = []\n",
    "tweets = {}\n",
    "tweet_count = 0\n",
    "\n",
    "# Start the search\n",
    "for status in twarc.search(target):\n",
    "\n",
    "# Output some status as we go, so we know something is happening\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(\"Collected \" + str(tweet_count) + \" tweets.\")\n",
    "    sys.stdout.flush()\n",
    "    tweet_count += 1\n",
    "\n",
    "    screen_name = None\n",
    "    if \"user\" in status:\n",
    "        if \"screen_name\" in status[\"user\"]:\n",
    "            screen_name = status[\"user\"][\"screen_name\"]\n",
    "\n",
    "    retweeted = retweeted_user(status)\n",
    "    if retweeted is not None:\n",
    "        influencer_frequency_dist[retweeted] += 1\n",
    "    else:\n",
    "        influencer_frequency_dist[screen_name] += 1\n",
    "\n",
    "# Tweet text can be in either \"text\" or \"full_text\" field...\n",
    "    text = None\n",
    "    if \"full_text\" in status:\n",
    "        text = status[\"full_text\"]\n",
    "    elif \"text\" in status:\n",
    "        text = status[\"text\"]\n",
    "\n",
    "    id_str = None\n",
    "    if \"id_str\" in status:\n",
    "        id_str = status[\"id_str\"]\n",
    "\n",
    "# Assemble the URL to the tweet we received...\n",
    "    tweet_url = None\n",
    "    if \"id_str\" is not None and \"screen_name\" is not None:\n",
    "        tweet_url = \"https://twitter.com/\" + screen_name + \"/status/\" + id_str\n",
    "\n",
    "# ...and capture it\n",
    "    if tweet_url is not None and text is not None:\n",
    "        tweets[tweet_url] = text\n",
    "\n",
    "# Record mapping graph between users\n",
    "    interactions = get_interactions(status)\n",
    "    if interactions is not None:\n",
    "        for user in interactions:\n",
    "            mentioned_frequency_dist[user] += 1\n",
    "            if screen_name not in user_user_graph:\n",
    "                user_user_graph[screen_name] = {}\n",
    "            if user not in user_user_graph[screen_name]:\n",
    "                user_user_graph[screen_name][user] = 1\n",
    "            else:\n",
    "                user_user_graph[screen_name][user] += 1\n",
    "\n",
    "# Record mapping graph between users and hashtags\n",
    "    hashtags = get_hashtags(status)\n",
    "    if hashtags is not None:\n",
    "        if len(hashtags) > 1:\n",
    "            hashtag_interactions = []\n",
    "\n",
    "# This code creates pairs of hashtags in situations where multiple\n",
    "# hashtags were found in a tweet\n",
    "# This is used to create a graph of hashtag-hashtag interactions\n",
    "\n",
    "    for comb in combinations(sorted(hashtags), 2):\n",
    "        hashtag_interactions.append(comb)\n",
    "    if len(hashtag_interactions) > 0:\n",
    "        for inter in hashtag_interactions:\n",
    "            item1, item2 = inter\n",
    "            if item1 not in hashtag_hashtag_graph:\n",
    "                hashtag_hashtag_graph[item1] = {}\n",
    "            if item2 not in hashtag_hashtag_graph[item1]:\n",
    "                hashtag_hashtag_graph[item1][item2] = 1\n",
    "            else:\n",
    "                hashtag_hashtag_graph[item1][item2] += 1\n",
    "\n",
    "    for hashtag in hashtags:\n",
    "        hashtag_frequency_dist[hashtag] += 1\n",
    "        if screen_name not in user_hashtag_graph:\n",
    "            user_hashtag_graph[screen_name] = {}\n",
    "        if hashtag not in user_hashtag_graph[screen_name]:\n",
    "            user_hashtag_graph[screen_name][hashtag] = 1\n",
    "        else:\n",
    "            user_hashtag_graph[screen_name][hashtag] += 1\n",
    "\n",
    "    urls = get_urls(status)\n",
    "    if urls is not None:\n",
    "        for url in urls:\n",
    "            url_frequency_dist[url] += 1\n",
    "\n",
    "    image_urls = get_image_urls(status)\n",
    "    if image_urls is not None:\n",
    "        for url in image_urls:\n",
    "            if url not in all_image_urls:\n",
    "                all_image_urls.append(url)\n",
    "\n",
    "# Iterate through image URLs, fetching each image if we haven't already\n",
    "    #print\n",
    "    print(\"Fetching images.\")\n",
    "    pictures_dir = os.path.join(save_dir, \"images\")\n",
    "\n",
    "    if not os.path.exists(pictures_dir):\n",
    "        print(\"Creating directory: \" + pictures_dir)\n",
    "        os.makedirs(pictures_dir)\n",
    "\n",
    "    for url in all_image_urls:\n",
    "        m = re.search(\"^http:\\/\\/pbs\\.twimg\\.com\\/media\\/(.+)$\", url)\n",
    "\n",
    "        if m is not None:\n",
    "            filename = m.group(1)\n",
    "            print(\"Getting picture from: \" + url)\n",
    "            save_path = os.path.join(pictures_dir, filename)\n",
    "\n",
    "            if not os.path.exists(save_path):\n",
    "                response = requests.get(url, stream=True)\n",
    "                with open(save_path, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "                del response\n",
    "\n",
    "# Output a bunch of files containing the data we just gathered\n",
    "    print(\"Saving data.\")\n",
    "    json_outputs = {\"tweets.json\": tweets,\n",
    "                    \"urls.json\": url_frequency_dist,\n",
    "                    \"hashtags.json\": hashtag_frequency_dist,\n",
    "                    \"influencers.json\": influencer_frequency_dist,\n",
    "                    \"mentioned.json\": mentioned_frequency_dist,\n",
    "                    \"user_user_graph.json\": user_user_graph,\n",
    "                    \"user_hashtag_graph.json\": user_hashtag_graph,\n",
    "                    \"hashtag_hashtag_graph.json\": hashtag_hashtag_graph}\n",
    "    for name, dataset in json_outputs.iteritems():\n",
    "        filename = os.path.join(save_dir, name)\n",
    "        save_json(dataset, filename)\n",
    "\n",
    "# These files are created in a format that can be easily imported into Gephi\n",
    "    csv_outputs = {\"user_user_graph.csv\": user_user_graph,\n",
    "                 \"user_hashtag_graph.csv\": user_hashtag_graph,\n",
    "                 \"hashtag_hashtag_graph.csv\": hashtag_hashtag_graph}\n",
    "    for name, dataset in csv_outputs.iteritems():\n",
    "        filename = os.path.join(save_dir, name)\n",
    "        save_csv(dataset, filename)\n",
    "\n",
    "    text_outputs = {\"hashtags.txt\": hashtag_frequency_dist,\n",
    "                  \"influencers.txt\": influencer_frequency_dist,\n",
    "                  \"mentioned.txt\": mentioned_frequency_dist,\n",
    "                  \"urls.txt\": url_frequency_dist}\n",
    "    for name, dataset in text_outputs.iteritems():\n",
    "        filename = os.path.join(save_dir, name)\n",
    "        save_text(dataset, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
