{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Import nltk stopwords and spacy for lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for Gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "\n",
    "import time\n",
    "\n",
    "import bs4\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import textacy\n",
    "\n",
    "from bokeh.charts import Chord\n",
    "\n",
    "from pysankey import sankey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dutch elections dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to load and preprocess the data. All Tweets were collected and stored in _JSON_ (JavaScript Object Notation) format. As reference: \"JSON is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate.\". After loading it, we want to manipulate the data in _lists_ and _dictionaries_. In addition, we will collect statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect names of Tweet files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import json\n",
    "mypath = 'data/election'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect all Tweets available and store them as list of dicts\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "for f in onlyfiles:\n",
    "    full_name = mypath+'/'+f\n",
    "\n",
    "    with open(full_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        all_tweets.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets_df = pd.DataFrame(all_tweets)\n",
    "all_tweets_df = all_tweets_df[all_tweets_df['lang'] == 'nl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('All tweets :',len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = all_tweets_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('All dutch only tweets :',len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save collected Tweets\n",
    "with open('elections_tweets', 'w') as fout:\n",
    "    json.dump(all_tweets, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load collected Tweets\n",
    "import json\n",
    "with open('elections_tweets', 'r') as fout:\n",
    "        all_tweets = json.load(fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "print('Sample of a Tweet:', all_tweets[0]['text'], '\\n')\n",
    "print('Number of fields available:', len(all_tweets[0].keys()), '\\n')\n",
    "print('Fields available in a Tweet:', '\\n', [*all_tweets[0]], '\\n')\n",
    "print('Info avaialable in the whole set:', '\\n', pd.DataFrame(all_tweets[:10]).columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the data (id and text), and put it into dict and list\n",
    "text_dict = {} \n",
    "text_list = []\n",
    "id_list = []\n",
    "missing_tweets = 0\n",
    "\n",
    "for tweet in all_tweets:\n",
    "    if ('text' in tweet.keys()):\n",
    "        if ('id' in tweet.keys()):\n",
    "            my_id = tweet['id']\n",
    "            if (tweet['text'] != None):\n",
    "                text_dict[my_id] = tweet['text']\n",
    "                id_list.append(my_id)\n",
    "                text_list.append([tweet['text']])\n",
    "                \n",
    "            elif (tweet['text'] == None):\n",
    "                missing_values = missing_tweets + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save processed Tweets\n",
    "\n",
    "\n",
    "with open('elections_text_list', 'wb') as fp:\n",
    "    pickle.dump(text_list, fp)\n",
    "    \n",
    "with open('elections_text_dict', 'wb') as fp:\n",
    "    pickle.dump(text_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load processed Tweets\n",
    "with open('elections_text_list', 'rb') as fp:\n",
    "    text_list = pickle.load(fp)\n",
    "    \n",
    "with open('elections_text_dict', 'rb') as fp:\n",
    "    text_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "t = len(text_dict)\n",
    "max_length = 0\n",
    "average_length = 0\n",
    "\n",
    "for tweet in text_list : \n",
    "    average_length = average_length + sum(len(i) for i in tweet) \n",
    "    if (sum(len(i) for i in tweet) > max_length) : \n",
    "        max_length = sum(len(i) for i in tweet)\n",
    "        max_tweet = tweet\n",
    "\n",
    "print('In total ' + str(t) + ' Tweets collected')\n",
    "print('On average Tweet is ' + str(round(average_length / t)) + ' charachters long', '\\n')\n",
    "print('Longest Tweet is:', max_tweet, '\\n')\n",
    "print('It is', sum(len(i) for i in max_tweet), 'characters long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's inspect the Tweets\n",
    "pd.DataFrame(text_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove emails and newline characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to preprocess the Tweets. We will exclude some symbols, links, etc. Thereafter for algorithmic purposes, we need to split each Tweet (a combination of sentences) into a combination of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before\n",
    "pprint(text_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(text):\n",
    "    soup =  bs4.BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "        \n",
    "    return bom_removed\n",
    "\n",
    "data = text_list\n",
    "# data = text_list[-int(1e5):]\n",
    "\n",
    "# Remove Emails and links to users\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "# Remove https and links\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "data = [re.sub(combined_pat, \"\", str(sent)) for sent in data]\n",
    "\n",
    "data = [re.sub(r\"https\\S+\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "data = [re.sub(r\"www.[^ ]+\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "\n",
    "data = [tweet_cleaner(sent) for sent in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After\n",
    "pprint(data[:3])\n",
    "# TODO: take care of \\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenize words and Clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences) :\n",
    "    '''Split sentences into words'''\n",
    "    for sentence in sentences :\n",
    "        # The main function here is Gensim's simple_preprocess\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Let's explore what do we have now\n",
    "pd.DataFrame(data_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're continuing to preprocess the Tweets by excluding so-called stopwords - just a set of common used words such as 'and', 'so', etc. in English. NLTK package has a collections of stopwords for Dutch language as well. Also, we can extend the set by adding words that should be considered as redundant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK stop words collection\n",
    "stop_words = stopwords.words('dutch')\n",
    "\n",
    "# TODO: Extend stop words collection using native speaker knowledge\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stop_words.extend(['rt', 'htt', 'via'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The number of Dutch stop words avaialable:', len(stop_words), '\\n')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereafter, we need to identify combinations of words that frequently appear together. For these purposes, we need to create so-called 'bigrams ' and 'trigrams'. Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "\n",
    "# One of the most important parameters here is min_count \n",
    "# If set up, for example, as 5, then only those bigrams and trigrams will be stored that appear 5 times or more\n",
    "\n",
    "# Threshold is \n",
    "# TODO: set up various parameters for min_count and threshold\n",
    "start = time.time()\n",
    "bigram = Phrases(data_words, min_count=5, threshold=10) \n",
    "trigram = Phrases(bigram[data_words], threshold=10)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "end = time.time()\n",
    "print('Algorithm time =', round((end-start)/60), 'min')\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[5]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is converting a word to its root word. Spacy NLP package responsible for that. Thanks we have Dutch language version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts) :\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts) :\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts) :\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) :\n",
    "    texts_out = []\n",
    "    for sent in texts :\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "start = time.time()\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('nl', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Algorithm time =', round((end-start)/60), 'min')\n",
    "print(data_words[:1])\n",
    "print(data_words_nostops[:1])\n",
    "print(data_words_bigrams[:1])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open('elections_data_lemmatized', 'wb') as fp:\n",
    "    pickle.dump(data_lemmatized, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('elections_data_lemmatized', 'rb') as fp:\n",
    "    data_lemmatized = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "dictionary = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents\n",
    "# dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Corpus contains pairs - a unique id for each word in the document and its frequency (word_id, word_frequency).\n",
    "print(pd.DataFrame(corpus[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We also can check what was the word from the dictionary - dictionary\n",
    "pprint(corpus[:1])\n",
    "print(dictionary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Readable format\n",
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens (words + bigrams + trigrams): %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Build Gensim LDA model\n",
    "# start = time.time()\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, # corpus\n",
    "#                                            dictionary=dictionary, # dict\n",
    "#                                            num_topics=10, # number of topic to be extracted\n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1, #\n",
    "#                                            chunksize=100, # \n",
    "#                                            passes=10, # \n",
    "#                                            alpha='auto', # \n",
    "#                                            per_word_topics=True) # \n",
    "# end = time.time()\n",
    "# print('Algorithm time =', round((end-start)/60), 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim LDA with Multiprocessing\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "num_topics = 10\n",
    "chunksize = 100\n",
    "passes = 10\n",
    "%time lda_model = LdaMulticore(corpus=corpus, \\\n",
    "                       id2word=dictionary.id2token, \\\n",
    "                       num_topics=num_topics, \\\n",
    "                       random_state=100, \\\n",
    "                       chunksize=chunksize, \\\n",
    "                       passes=passes, \\\n",
    "                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model for further usage\n",
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath('lda_model_300k') \n",
    "lda_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a potentially pretrained model\n",
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath('lda_model_300k') \n",
    "lda_model = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Print the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print one topic\n",
    "lda_model.show_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print all topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure perfomance there are two metrics: perplexity and coherence score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Scores\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score c_v measure: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score UMass measure: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaMulticore(corpus=corpus, \n",
    "                       id2word=dictionary, \n",
    "                       num_topics=num_topics, \n",
    "                       random_state=100,\n",
    "                       chunksize=chunksize, \n",
    "                       passes=passes,\n",
    "                       per_word_topics=True)\n",
    "        print('Model with', num_topics, 'topics done')\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 3\n",
    "limit = 20\n",
    "step = 1\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=data_lemmatized, start=start, limit=limit, step=step)\n",
    "\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=limit; start=start; step=step;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can decrease the number of words per topic and show them separately\n",
    "\n",
    "topn = 10 # to keep it readable\n",
    "top_topics = lda_model.top_topics(corpus=corpus, dictionary=dictionary, topn=topn)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Visualize topics-keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bubble is a topic. The larger the bubble, the more prevalent the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
