{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bokeh\\util\\deprecation.py:34: BokehDeprecationWarning: \n",
      "The bokeh.charts API has moved to a separate 'bkcharts' package.\n",
      "\n",
      "This compatibility shim will remain until Bokeh 1.0 is released.\n",
      "After that, if you want to use this API you will have to install\n",
      "the bkcharts package explicitly.\n",
      "\n",
      "  warn(message)\n"
     ]
    }
   ],
   "source": [
    "import re, os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Import nltk stopwords and spacy for lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import spacy\n",
    "# Enable logging for Gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import bs4\n",
    "\n",
    "# Collect names of Tweet files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import textacy\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from bokeh.charts import Chord\n",
    "\n",
    "from ipysankeywidget import SankeyWidget\n",
    "from ipywidgets import Layout\n",
    "from IPython.display import display\n",
    "\n",
    "from pysankey import sankey\n",
    "from IPython.display import (\n",
    "    Image,\n",
    "    SVG\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing part (uncomment this section to do preprocessing part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mypath = 'data/election'\n",
    "# onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Collect all Tweets available and store them as list of dicts\n",
    "\n",
    "# all_tweets = []\n",
    "\n",
    "# for f in onlyfiles:\n",
    "#     full_name = mypath+'/'+f\n",
    "\n",
    "#     with open(full_name, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#         all_tweets.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_tweets_df = pd.DataFrame(all_tweets)\n",
    "## Select only dutch tweets\n",
    "# all_tweets_df = all_tweets_df[all_tweets_df['lang'] == 'nl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('All tweets :',len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_tweets = all_tweets_df.to_dict('records')\n",
    "# print('All dutch only tweets :',len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save collected Tweets\n",
    "# with open('elections_tweets', 'w') as fout:\n",
    "#     json.dump(all_tweets, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Loaded and pre-pre-processed collected Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Load collected Tweets\n",
    "# import json\n",
    "# with open('elections_tweets', 'r') as fout:\n",
    "#         all_tweets = json.load(fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Statistics\n",
    "# print('Sample of a Tweet:', all_tweets[0]['text'], '\\n')\n",
    "# print('Number of fields available:', len(all_tweets[0].keys()), '\\n')\n",
    "# print('Fields available in a Tweet:', '\\n', [*all_tweets[0]], '\\n')\n",
    "# print('Info avaialable in the whole set:', '\\n', pd.DataFrame(all_tweets[:10]).columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating tweet to list and dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Extract the data (id and text), and put it into dict and list\n",
    "# text_dict = {} \n",
    "# text_list = []\n",
    "# id_list = []\n",
    "# missing_tweets = 0\n",
    "\n",
    "# for tweet in all_tweets:\n",
    "#     if ('text' in tweet.keys()):\n",
    "#         if ('id' in tweet.keys()):\n",
    "#             my_id = tweet['id']\n",
    "#             if (tweet['text'] != None):\n",
    "#                 text_dict[my_id] = tweet['text']\n",
    "#                 id_list.append(my_id)\n",
    "#                 text_list.append([tweet['text']])\n",
    "                \n",
    "#             elif (tweet['text'] == None):\n",
    "#                 missing_values = missing_tweets + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save processed Tweets\n",
    "# with open('elections_text_list', 'wb') as fp:\n",
    "#     pickle.dump(text_list, fp)\n",
    "    \n",
    "# with open('elections_text_dict', 'wb') as fp:\n",
    "#     pickle.dump(text_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Loaded and pre-pre-processed text list and dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Load processed Tweets\n",
    "# with open('elections_text_list', 'rb') as fp:\n",
    "#     text_list = pickle.load(fp)\n",
    "    \n",
    "# with open('elections_text_dict', 'rb') as fp:\n",
    "#     text_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Text Statistics\n",
    "# t = len(text_dict)\n",
    "# max_length = 0\n",
    "# average_length = 0\n",
    "\n",
    "# for tweet in text_list : \n",
    "#     average_length = average_length + sum(len(i) for i in tweet) \n",
    "#     if (sum(len(i) for i in tweet) > max_length) : \n",
    "#         max_length = sum(len(i) for i in tweet)\n",
    "#         max_tweet = tweet\n",
    "\n",
    "# print('In total ' + str(t) + ' Tweets collected')\n",
    "# print('On average Tweet is ' + str(round(average_length / t)) + ' charachters long', '\\n')\n",
    "# print('Longest Tweet is:', max_tweet, '\\n')\n",
    "# print('It is', sum(len(i) for i in max_tweet), 'characters long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def tweet_cleaner(text):\n",
    "#     soup =  bs4.BeautifulSoup(text, 'lxml')\n",
    "#     souped = soup.get_text()\n",
    "#     try:\n",
    "#         bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "#     except:\n",
    "#         bom_removed = souped\n",
    "        \n",
    "#     return bom_removed\n",
    "\n",
    "# data = text_list\n",
    "# # data = text_list[-int(1e5):]\n",
    "\n",
    "# # Remove Emails and links to users\n",
    "# data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
    "\n",
    "# # Remove new line characters\n",
    "# data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n",
    "\n",
    "# # Remove distracting single quotes\n",
    "# data = [re.sub(\"\\'\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "# # Remove https and links\n",
    "# pat1 = r'@[A-Za-z0-9]+'\n",
    "# pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "# combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "# data = [re.sub(combined_pat, \"\", str(sent)) for sent in data]\n",
    "\n",
    "# data = [re.sub(r\"https\\S+\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "# data = [re.sub(r\"www.[^ ]+\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "\n",
    "# data = [tweet_cleaner(sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def sent_to_words(sentences) :\n",
    "#     '''Split sentences into words'''\n",
    "#     for sentence in sentences :\n",
    "#         # The main function here is Gensim's simple_preprocess\n",
    "#         yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# data_words = list(sent_to_words(data))\n",
    "\n",
    "# # Let's explore what do we have now\n",
    "# pd.DataFrame(data_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # NLTK stop words collection\n",
    "# stop_words = stopwords.words('dutch')\n",
    "\n",
    "# # TODO: Extend stop words collection using native speaker knowledge\n",
    "# # stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "# stop_words.extend(['rt', 'htt', 'via'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('The number of Dutch stop words avaialable:', len(stop_words), '\\n')\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Build the bigram and trigram models\n",
    "\n",
    "# # One of the most important parameters here is min_count \n",
    "# # If set up, for example, as 5, then only those bigrams and trigrams will be stored that appear 5 times or more\n",
    "\n",
    "# # Threshold is \n",
    "# # TODO: set up various parameters for min_count and threshold\n",
    "# start = time.time()\n",
    "# bigram = Phrases(data_words, min_count=5, threshold=10) \n",
    "# trigram = Phrases(bigram[data_words], threshold=10)  \n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "# bigram_mod = Phraser(bigram)\n",
    "# trigram_mod = Phraser(trigram)\n",
    "# end = time.time()\n",
    "# print('Algorithm time =', round((end-start)/60), 'min')\n",
    "\n",
    "# # See trigram example\n",
    "# print(trigram_mod[bigram_mod[data_words[5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# def remove_stopwords(texts) :\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# def make_bigrams(texts) :\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts) :\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) :\n",
    "#     texts_out = []\n",
    "#     for sent in texts :\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "#     return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Remove stop words\n",
    "# start = time.time()\n",
    "# data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# # Form bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# nlp = spacy.load('nl', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save results\n",
    "# with open('elections_data_lemmatized', 'wb') as fp:\n",
    "#     pickle.dump(data_lemmatized, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the cleaned preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('elections_data_lemmatized', 'rb') as fp:\n",
    "    data_lemmatized = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get topic from LDA model (Looks into mikhael work for this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = LdaModel.load('lda_model_2d')\n",
    "all_topic = []\n",
    "for i in range(lda_model.num_topics):\n",
    "    all_topic += [lda_model.show_topic(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate score and words inside each topic for further processing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_num_sep (all_topic = all_topic):\n",
    "    \n",
    "    all_topic_words = []\n",
    "    all_topic_num = []\n",
    "\n",
    "    for i in range(len(all_topic)):\n",
    "        one_topic_words = []\n",
    "        one_topic_num = []\n",
    "        for j in range(len(all_topic[i])):\n",
    "            one_topic_words += [all_topic[i][j][0]]\n",
    "            one_topic_num += [all_topic[i][j][1]]\n",
    "\n",
    "        all_topic_words += [one_topic_words]\n",
    "        all_topic_num += [one_topic_num]\n",
    "    \n",
    "    return all_topic_words,all_topic_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_topic_words,all_topic_num = score_num_sep (all_topic = all_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group each words inside each topics \n",
    "    \n",
    "    - The input is \"Topic Modelling\" topics that consists of list of words with the score (LDA, words and score)\n",
    "    - Moving standard deviation was used for checking whether which words belong to which group inside the topic\n",
    "    - The output is grouped list of words only, scores only and words and scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_grouping (all_topic = all_topic,all_topic_words=all_topic_words,all_topic_num=all_topic_num):\n",
    "\n",
    "    all_group_topic_all = []\n",
    "    all_group_topic_words = []\n",
    "    all_group_topic_num = []\n",
    "\n",
    "    for topic in range (len(all_topic)):\n",
    "        all_group_all = []\n",
    "        all_group_words = []\n",
    "        all_group_num = []\n",
    "        count = 0\n",
    "\n",
    "        while count < len(all_topic[topic]):\n",
    "\n",
    "            single_group_all = all_topic[topic][count]\n",
    "            single_group_words_only = (all_topic_words[topic][count],)\n",
    "            single_group_num_only = (all_topic_num[topic][count],)\n",
    "            \n",
    "            # using moving standard deviatiaon for checking group\n",
    "            range_a = abs(all_topic_num[topic][count] - np.array(all_topic_num[topic][count:len(all_topic_num[topic])]).std())\n",
    "\n",
    "            for j in range(count,len(all_topic[topic])-1):  \n",
    "\n",
    "                if count == (len(all_topic[topic])-1):\n",
    "                    if all_topic_num[topic][-1] <= range_a:\n",
    "                        single_group_all = (single_group_all) + (all_topic[topic][-1])\n",
    "                        single_group_words_only = tuple(list(single_group_words_only) + ([all_topic_words[topic][-1]]))\n",
    "                        single_group_num_only = tuple(list(single_group_num_only) + ([all_topic_num[topic][-1]])) \n",
    "\n",
    "                elif count < (len(all_topic[topic])-1):       \n",
    "                    if all_topic_num[topic][count+1] >= range_a:\n",
    "                        single_group_all = (single_group_all) + (all_topic[topic][count+1])\n",
    "                        single_group_words_only = tuple(list(single_group_words_only) + ([all_topic_words[topic][count+1]]))\n",
    "                        single_group_num_only = tuple(list(single_group_num_only) + ([all_topic_num[topic][count+1]])) \n",
    "                        count += 1    \n",
    "\n",
    "            all_group_all += [single_group_all]\n",
    "            all_group_words += [single_group_words_only]\n",
    "            all_group_num += [single_group_num_only]\n",
    "            count += 1\n",
    "\n",
    "        all_group_topic_all += [all_group_all]\n",
    "        all_group_topic_words += [all_group_words]\n",
    "        all_group_topic_num += [all_group_num]\n",
    "        \n",
    "    return all_group_topic_all, all_group_topic_words, all_group_topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_group_topic_all, all_group_topic_words, all_group_topic_num = topic_grouping (all_topic=all_topic,\n",
    "                                                                                  all_topic_words=all_topic_words,\n",
    "                                                                                  all_topic_num=all_topic_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sankey diagram layout settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layout = Layout(width=\"1000\", height=\"200\")\n",
    "def sankey(margin_top=10, **value):\n",
    "    \"\"\"Show SankeyWidget with default values for size and margins\"\"\"\n",
    "    return SankeyWidget(layout=layout,\n",
    "                        margins=dict(top=margin_top, bottom=0, left=100, right=100),\n",
    "                        **value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataset list of dicts for sankey diagram using grouped words in the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sankey_links_dict(all_group_topic_words=all_group_topic_words):\n",
    "    \n",
    "    links_all = []\n",
    "    # Colour list dictionary, please add more list (copy paste list or add rows of list) \n",
    "    # and colour (add collectiion of list) if needed to produce more colour or if error (out of range) occur\n",
    "    colour = [\n",
    "              ['green','lime', 'olive','gold', 'orange','coral', 'red','crimson', 'pink','deeppink','fuchsia', 'lightskyblue',\n",
    "              'blueviolet','cornflowerblue','blue','navy'], \n",
    "              ['coral', 'red','crimson', 'pink','deeppink','green','lime', 'olive','gold', 'orange','fuchsia', 'lightskyblue',\n",
    "              'blueviolet','cornflowerblue','blue','navy'],\n",
    "              ['fuchsia','lightskyblue','blueviolet','cornflowerblue','blue','navy','green','lime', 'olive','gold', 'orange',\n",
    "               'coral', 'red','crimson', 'pink','deeppink'], \n",
    "              ['crimson', 'pink','cornflowerblue','blue','navy','green','lime', 'olive','gold', 'orange','coral', 'red',\n",
    "               'deeppink','fuchsia', 'lightskyblue','blueviolet'],\n",
    "              ['gold', 'orange','coral''green','lime''blueviolet','cornflowerblue', 'olive', 'red','crimson', 'pink','deeppink',\n",
    "               'fuchsia', 'lightskyblue','blue','navy'],\n",
    "              ['crimson', 'pink','cornflowerblue','blue','navy','green','lime', 'olive','gold', 'orange','coral', 'red',\n",
    "               'deeppink','fuchsia', 'lightskyblue','blueviolet'], \n",
    "              ['coral', 'red','crimson', 'pink','deeppink','green','lime', 'olive','gold', 'orange','fuchsia', 'lightskyblue',\n",
    "              'blueviolet','cornflowerblue','blue','navy'],\n",
    "              \n",
    "             ]\n",
    "    \n",
    "    # cont the number of all topic\n",
    "    topic_num_count = len(all_group_topic_words)\n",
    "    \n",
    "    # creating links between words by checking the number of words inside the previous and next group of words and link them\n",
    "    for a in range(topic_num_count):\n",
    "        groups_count = len(all_group_topic_words[a])\n",
    "        links = []\n",
    "        for b in range(groups_count-1):\n",
    "            \n",
    "            # check and record number of group of words in current and next\n",
    "            words_count_1st = len(all_group_topic_words[a][b])\n",
    "            words_count_2nd = len(all_group_topic_words[a][b+1])\n",
    "\n",
    "            for c in range(words_count_1st):\n",
    "                for d in range(words_count_2nd):\n",
    "                    \n",
    "                    # creating the dict for sankey diagram visualization\n",
    "                    dict_1st = {'source': all_group_topic_words[a][b][c], \n",
    "                                'target': all_group_topic_words[a][b+1][d], \n",
    "                                'value': max(6/words_count_2nd,6/words_count_1st), # use the maximum links values with maximum 6 \n",
    "                                'color': colour[c][b]} # the use of colour list dictionary \n",
    "                    \n",
    "                    links = links + [dict_1st]\n",
    "\n",
    "        links_all = links_all + [links]\n",
    "        \n",
    "    return links_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating sankey links for each words in LDA topics\n",
    "\n",
    "links_all = sankey_links_dict(all_group_topic_words=all_group_topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 1 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb86e98c3db24a1888afb556c87bd4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'ollongren', 'target': 'gewoon', 'va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[0]).auto_save_png('1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 2 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f777ea48c8429aa22536a174c487d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'democratie', 'target': 'forum', 'va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[1]).auto_save_png('2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 3 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acd29d3cb774ec8910ea4f59e7da8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'vvd', 'target': 'partij', 'value': …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[2]).auto_save_png('3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 4 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9e4830357649dd8ca5d4c1da107a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'vvd', 'target': 'pvda', 'value': 6.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[3]).auto_save_png('4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 5 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544b4ad8134542ac9c9ced2e4d287cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'pvv', 'target': 'stem', 'value': 6.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[4]).auto_save_png('5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 6 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe7c996677344adafc2017613bcb64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'vvd', 'target': 'zijlstra', 'value'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[5]).auto_save_png('6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 7 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5841664b2a43f798feed055d29a0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'vvd', 'target': 'wel', 'value': 3.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[6]).auto_save_png('7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 8 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a288ac332d4269abbccfd9ff177d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'pvda', 'target': 'rotterdam', 'valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[7]).auto_save_png('8.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 9 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9708b4ef1a42e79af386b287f1507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'cda', 'target': 'vvd', 'value': 6.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[8]).auto_save_png('9.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 10 words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065ac4400c1f48b38ad739e2afae7137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'nederland', 'target': 'christenunie…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sankey(links=links_all[9]).auto_save_png('10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training documents (all tweets) for finding alternative topic (Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Document training with doc2vec\n",
    "\n",
    "## Tag each tweets \n",
    "# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data_lemmatized)]\n",
    "\n",
    "## Train documents(tweets) with doc2vec library\n",
    "# model = Doc2Vec(documents, vector_size=300, window_size = 15, min_count=1, epochs=150, \n",
    "#                 sampling_threshold = 1e-5,negative_size = 5, \n",
    "#                 workers=32,train_epoch = 150, dm = 0) #0 = dbow; 1 = dmpv \n",
    "\n",
    "## Save the training set model for further use\n",
    "# model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the training set model\n",
    "model = Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use topics that were found in \"Topic Analysis\" (LDA) to find similiar topic that match with traininng set model in doc2vec\n",
    "def similar_topic_doc2vec(word_num=15, all_topic_words = all_topic_words,model=model):\n",
    "    similar_topic = []\n",
    "\n",
    "    for topic in range(len(all_topic_words)):\n",
    "        \n",
    "        positive_words = list(all_topic_words[topic])\n",
    "        \n",
    "        # find words that is not really match with the rest of the words in the topic \n",
    "        # and set that as negative words of the topic\n",
    "        negative_words = model.wv.doesnt_match(all_topic_words[topic])\n",
    "        \n",
    "        # set positive words as all of the words in the topic except the negative words\n",
    "        positive_words.remove(negative_words)\n",
    "        \n",
    "        # set the numbers of words for alternative topic that will be generated\n",
    "        branch_num = word_num\n",
    "        \n",
    "        # function to find the most coresponding topics according to the positive and negative words input\n",
    "        similar = model.wv.most_similar_cosmul(positive =  positive_words,\n",
    "                                     negative = [negative_words],topn=branch_num)\n",
    "\n",
    "        similar_topic = similar_topic + [similar]\n",
    "        \n",
    "    return similar_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply find alternatives topic (we can change the words number!)\n",
    "similar_topic = similar_topic_doc2vec(word_num=12, all_topic_words = all_topic_words,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate words and number inside the topics for the alternative topics\n",
    "all_similar_topic_words,all_similar_topic_num = score_num_sep (all_topic = similar_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words grouping (clustering) for alternative topics\n",
    "all_similar_group_topic_all, all_similar_group_topic_words, all_similar_group_topic_num = topic_grouping (all_topic=similar_topic,\n",
    "                                                                                                          all_topic_words=all_similar_topic_words,\n",
    "                                                                                                          all_topic_num=all_similar_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating sankey links for each words in alternative topics\n",
    "similar_links_all = sankey_links_dict(all_group_topic_words=all_similar_group_topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d64fc2a6d8c4dbfbed1274d2da58fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'ollongren_ngesteund', 'target': 'ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 1\n",
    "sankey(links=similar_links_all[0]).auto_save_png('alt_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1864c2dad96247bf810bbf2ec8a364d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'entameren', 'target': 'blauwe_ogen'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 2\n",
    "sankey(links=similar_links_all[1]).auto_save_png('alt_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9346c137128b4dc5be7b8ebad3566540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'leukst', 'target': 'pvd', 'value': …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 3\n",
    "sankey(links=similar_links_all[2]).auto_save_png('alt_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c469eca7a5041038dddf93b632c594b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'derhalve', 'target': 'golddigger', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 4\n",
    "sankey(links=similar_links_all[3]).auto_save_png('alt_4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3568888d1dc4d258a5ef6b76a1b5677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'muissti', 'target': 'nbilan', 'valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 5\n",
    "sankey(links=similar_links_all[4]).auto_save_png('alt_5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1102ad0ed646ca92500ae0ceda20f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'kanes', 'target': 'derde_termijn', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 6\n",
    "sankey(links=similar_links_all[5]).auto_save_png('alt_6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea503156fc74bc99a501afd639c0500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'kaaimaneilanden', 'target': 'weeeee…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 7\n",
    "sankey(links=similar_links_all[6]).auto_save_png('alt_7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12137a4d24c44177be0933603007cd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'nhuishoudelij', 'target': 'wilders_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 8\n",
    "sankey(links=similar_links_all[7]).auto_save_png('alt_8.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3beeb0ecdd0e4f14ab341a051b39250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'beschaamd', 'target': 'nlach', 'val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 9\n",
    "sankey(links=similar_links_all[8]).auto_save_png('alt_9.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Topic for LDA Topic 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5343e386b004e0282974d90bed2f9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'minst_belangrijke', 'target': 'maar…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 10\n",
    "sankey(links=similar_links_all[9]).auto_save_png('alt_10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specific = [model.wv.most_similar_cosmul(positive = ['fvd','racisme','baudet'] ,topn=15),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate words and number inside the topics for the alternative topics\n",
    "specific_words,specific_num = score_num_sep (all_topic = specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sankey links for each words in alternative topics\n",
    "# words grouping (clustering) for alternative topics\n",
    "all_specific_all, all_specific_words, all_specific_num = topic_grouping (all_topic=specific,\n",
    "                                                                         all_topic_words=specific_words,\n",
    "                                                                         all_topic_num=specific_num)\n",
    "similar_links_all = sankey_links_dict(all_group_topic_words=all_specific_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6e7b4351284a55a72d656bbefba773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='200', width='1000'), links=[{'source': 'derijnpost', 'target': 'nnassaulaan…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alternative topic for LDA topic 10\n",
    "sankey(links=similar_links_all[0]).auto_save_png('specific_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
