{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Import nltk stopwords and spacy for lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for Gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of words\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = 'C:/Users/Mikhail/Documents/GitHub/topic_modeling/data/elections'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect all Tweets available and store them as list of dicts\n",
    "import json\n",
    "all_tweets = []\n",
    "\n",
    "for f in onlyfiles:\n",
    "    full_name = mypath+'/'+f\n",
    "\n",
    "    with open(full_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        all_tweets.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys assigned: 28 \n",
      "\n",
      "Sample of a Tweet: RT @Derksen_Gelul: Nederlands bekendste droogkloot, Marcel van Roosmalen sloopt D66 helemaal de moeder!! Luister zelf naar deze topper en g… \n",
      "\n",
      "Info available in a Tweet: \n",
      " dict_keys(['created_at', 'id', 'id_str', 'text', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'retweeted_status', 'is_quote_status', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms'])\n",
      "\n",
      " Info avaialable in the whole set: \n",
      " ['contributors' 'coordinates' 'created_at' 'display_text_range' 'entities'\n",
      " 'extended_tweet' 'favorite_count' 'favorited' 'filter_level' 'geo' 'id'\n",
      " 'id_str' 'in_reply_to_screen_name' 'in_reply_to_status_id'\n",
      " 'in_reply_to_status_id_str' 'in_reply_to_user_id'\n",
      " 'in_reply_to_user_id_str' 'is_quote_status' 'lang' 'place'\n",
      " 'possibly_sensitive' 'quote_count' 'quoted_status' 'quoted_status_id'\n",
      " 'quoted_status_id_str' 'reply_count' 'retweet_count' 'retweeted'\n",
      " 'retweeted_status' 'source' 'text' 'timestamp_ms' 'truncated' 'user']\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print('Number of keys assigned:', len(all_tweets[0].keys()), '\\n')\n",
    "print('Sample of a Tweet:', all_tweets[0]['text'], '\\n')\n",
    "print('Info available in a Tweet:', '\\n', all_tweets[0].keys())\n",
    "print('\\n','Info avaialable in the whole set:', '\\n', pd.DataFrame(all_tweets[:10]).columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the data (id and text), and put it into dict and list\n",
    "text_dict = {} \n",
    "text_list = []\n",
    "id_list = []\n",
    "missing_tweets = 0\n",
    "\n",
    "for tweet in all_tweets:\n",
    "    if ('text' in tweet.keys()):\n",
    "        if ('id' in tweet.keys()):\n",
    "            my_id = tweet['id']\n",
    "            if (tweet['text'] != None):\n",
    "                text_dict[my_id] = tweet['text']\n",
    "                id_list.append(my_id)\n",
    "                text_list.append([tweet['text']])\n",
    "                \n",
    "            elif (tweet['text'] == None):\n",
    "                missing_values = missing_tweets + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numver of blank Tweets = 0\n",
      "In total 795867 Tweets collected\n",
      "On average Tweet is 124 charachters long \n",
      "\n",
      "Longest Tweet is: ['@tacticsoftuchel Courtois&gt;Karius\\nAzpi&gt;Clyne \\nChristensen&lt;VVD (for now)\\nZouma/Rudiger&gt;&gt;&gt;&gt;&gt;&gt;\\nEmerson/Alonso&lt;Robertson… https://t.co/5YNNJog70t'] \n",
      "\n",
      "It is 170 characters long\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "t = len(text_dict)\n",
    "max_length = 0\n",
    "average_length = 0\n",
    "\n",
    "for tweet in text_list : \n",
    "    average_length = average_length + sum(len(i) for i in tweet) \n",
    "    if (sum(len(i) for i in tweet) > max_length) : \n",
    "        max_length = sum(len(i) for i in tweet)\n",
    "        max_tweet = tweet\n",
    "\n",
    "print('The numver of blank Tweets = ' + str(missing_tweets))\n",
    "print('In total ' + str(t) + ' Tweets collected')\n",
    "print('On average Tweet is ' + str(round(average_length / t)) + ' charachters long', '\\n')\n",
    "print('Longest Tweet is:', max_tweet, '\\n')\n",
    "print('It is', sum(len(i) for i in max_tweet), 'characters long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Derksen_Gelul: Nederlands bekendste droogk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @GerBStruik: Niet alleen Ollongren, maar ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JoostNiemoller Joost, trap er toch niet in. H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@EenVandaag @thierrybaudet @D66 @APechtold Ald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dolhuysbrug moet na tien jaar definitief van t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  RT @Derksen_Gelul: Nederlands bekendste droogk...\n",
       "1  RT @GerBStruik: Niet alleen Ollongren, maar ni...\n",
       "2  @JoostNiemoller Joost, trap er toch niet in. H...\n",
       "3  @EenVandaag @thierrybaudet @D66 @APechtold Ald...\n",
       "4  Dolhuysbrug moet na tien jaar definitief van t..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's inspect the Tweets\n",
    "pd.DataFrame(text_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove emails and newline characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['RT @Derksen_Gelul: Nederlands bekendste droogkloot, Marcel van Roosmalen '\n",
      "  'sloopt D66 helemaal de moeder!! Luister zelf naar deze topper en g…'],\n",
      " ['RT @GerBStruik: Niet alleen Ollongren, maar niemand in #Rutte3 heeft '\n",
      "  'kennelijk enig benul van staatsrecht. Een minister die als minister ee…'],\n",
      " ['@JoostNiemoller Joost, trap er toch niet in. Hier is één grote minne '\n",
      "  'manipulatie aan de gang tegen FVD. Deze heeft… https://t.co/lVFOuFFYF3']]\n"
     ]
    }
   ],
   "source": [
    "# Before\n",
    "pprint(text_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = df.content.values.tolist()\n",
    "# TODO: include other Tweets into analysis\n",
    "data = text_list[:10000]\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "# TODO: remove https and links\n",
    "data = [re.sub(\"https\", \"\", str(sent)) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[RT Nederlands bekendste droogkloot, Marcel van Roosmalen sloopt D66 '\n",
      " 'helemaal de moeder!! Luister zelf naar deze topper en g…]',\n",
      " '[RT Niet alleen Ollongren, maar niemand in #Rutte3 heeft kennelijk enig '\n",
      " 'benul van staatsrecht. Een minister die als minister ee…]',\n",
      " 'Joost, trap er toch niet in. Hier is één grote minne manipulatie aan de gang '\n",
      " 'tegen FVD. Deze heeft… ://t.co/lVFOuFFYF3]']\n"
     ]
    }
   ],
   "source": [
    "# After\n",
    "pprint(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenize words and Clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt</td>\n",
       "      <td>nederlands</td>\n",
       "      <td>bekendste</td>\n",
       "      <td>droogkloot</td>\n",
       "      <td>marcel</td>\n",
       "      <td>van</td>\n",
       "      <td>roosmalen</td>\n",
       "      <td>sloopt</td>\n",
       "      <td>helemaal</td>\n",
       "      <td>de</td>\n",
       "      <td>...</td>\n",
       "      <td>luister</td>\n",
       "      <td>zelf</td>\n",
       "      <td>naar</td>\n",
       "      <td>deze</td>\n",
       "      <td>topper</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt</td>\n",
       "      <td>niet</td>\n",
       "      <td>alleen</td>\n",
       "      <td>ollongren</td>\n",
       "      <td>maar</td>\n",
       "      <td>niemand</td>\n",
       "      <td>in</td>\n",
       "      <td>rutte</td>\n",
       "      <td>heeft</td>\n",
       "      <td>kennelijk</td>\n",
       "      <td>...</td>\n",
       "      <td>benul</td>\n",
       "      <td>van</td>\n",
       "      <td>staatsrecht</td>\n",
       "      <td>een</td>\n",
       "      <td>minister</td>\n",
       "      <td>die</td>\n",
       "      <td>als</td>\n",
       "      <td>minister</td>\n",
       "      <td>ee</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joost</td>\n",
       "      <td>trap</td>\n",
       "      <td>er</td>\n",
       "      <td>toch</td>\n",
       "      <td>niet</td>\n",
       "      <td>in</td>\n",
       "      <td>hier</td>\n",
       "      <td>is</td>\n",
       "      <td>een</td>\n",
       "      <td>grote</td>\n",
       "      <td>...</td>\n",
       "      <td>manipulatie</td>\n",
       "      <td>aan</td>\n",
       "      <td>de</td>\n",
       "      <td>gang</td>\n",
       "      <td>tegen</td>\n",
       "      <td>fvd</td>\n",
       "      <td>deze</td>\n",
       "      <td>heeft</td>\n",
       "      <td>co</td>\n",
       "      <td>lvfouffyf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aldus</td>\n",
       "      <td>leugenaar</td>\n",
       "      <td>hoe</td>\n",
       "      <td>durft</td>\n",
       "      <td>deze</td>\n",
       "      <td>partij</td>\n",
       "      <td>nog</td>\n",
       "      <td>de</td>\n",
       "      <td>van</td>\n",
       "      <td>co</td>\n",
       "      <td>...</td>\n",
       "      <td>kqfumv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dolhuysbrug</td>\n",
       "      <td>moet</td>\n",
       "      <td>na</td>\n",
       "      <td>tien</td>\n",
       "      <td>jaar</td>\n",
       "      <td>definitief</td>\n",
       "      <td>van</td>\n",
       "      <td>tafel</td>\n",
       "      <td>vvd</td>\n",
       "      <td>haarlem</td>\n",
       "      <td>...</td>\n",
       "      <td>saanwvxg</td>\n",
       "      <td>nmaak</td>\n",
       "      <td>uw</td>\n",
       "      <td>eigen</td>\n",
       "      <td>filter</td>\n",
       "      <td>ove</td>\n",
       "      <td>co</td>\n",
       "      <td>pbveexkhtf</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1          2           3       4           5   \\\n",
       "0           rt  nederlands  bekendste  droogkloot  marcel         van   \n",
       "1           rt        niet     alleen   ollongren    maar     niemand   \n",
       "2        joost        trap         er        toch    niet          in   \n",
       "3        aldus   leugenaar        hoe       durft    deze      partij   \n",
       "4  dolhuysbrug        moet         na        tien    jaar  definitief   \n",
       "\n",
       "          6       7         8          9     ...               11     12  \\\n",
       "0  roosmalen  sloopt  helemaal         de    ...          luister   zelf   \n",
       "1         in   rutte     heeft  kennelijk    ...            benul    van   \n",
       "2       hier      is       een      grote    ...      manipulatie    aan   \n",
       "3        nog      de       van         co    ...           kqfumv   None   \n",
       "4        van   tafel       vvd    haarlem    ...         saanwvxg  nmaak   \n",
       "\n",
       "            13     14        15    16    17          18    19         20  \n",
       "0         naar   deze    topper    en  None        None  None       None  \n",
       "1  staatsrecht    een  minister   die   als    minister    ee       None  \n",
       "2           de   gang     tegen   fvd  deze       heeft    co  lvfouffyf  \n",
       "3         None   None      None  None  None        None  None       None  \n",
       "4           uw  eigen    filter   ove    co  pbveexkhtf  None       None  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent_to_words(sentences) :\n",
    "    '''Split sentences into words'''\n",
    "    for sentence in sentences :\n",
    "        # The main function here is Gensim's simple_preprocess\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Let's explore what do we have now\n",
    "pd.DataFrame(data_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK stop words collection\n",
    "stop_words = stopwords.words('dutch')\n",
    "\n",
    "# TODO: Extend stop words collection using native speaker knowledge\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Dutch stop words avaialable: 101\n"
     ]
    }
   ],
   "source": [
    "print('The number of Dutch stop words avaialable:', len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'niet', 'alleen_ollongren', 'maar', 'niemand', 'in', 'rutte', 'heeft', 'kennelijk_enig_benul', 'van', 'staatsrecht', 'een', 'minister', 'die', 'als', 'minister_ee']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts) :\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts) :\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts) :\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) :\n",
    "    texts_out = []\n",
    "    for sent in texts :\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is converting a word to its root word. Spacy NLP package responsible for that. Thanks we have Dutch language version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nederlands_bekendste', 'droogkloot_marcel', 'roosmalen_sloopt', 'helemaal', 'moeder_luister', 'topper']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('nl', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', 'nederlands', 'bekendste', 'droogkloot', 'marcel', 'van', 'roosmalen', 'sloopt', 'helemaal', 'de', 'moeder', 'luister', 'zelf', 'naar', 'deze', 'topper', 'en']]\n"
     ]
    }
   ],
   "source": [
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', 'nederlands', 'bekendste', 'droogkloot', 'marcel', 'roosmalen', 'sloopt', 'helemaal', 'moeder', 'luister', 'topper']]\n"
     ]
    }
   ],
   "source": [
    "print(data_words_nostops[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', 'nederlands_bekendste', 'droogkloot_marcel', 'roosmalen_sloopt', 'helemaal', 'moeder_luister', 'topper']]\n"
     ]
    }
   ],
   "source": [
    "print(data_words_bigrams[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nederlands_bekendste', 'droogkloot_marcel', 'roosmalen_sloopt', 'helemaal', 'moeder_luister', 'topper']]\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0       1       2       3       4       5\n",
      "0  (0, 1)  (1, 1)  (2, 1)  (3, 1)  (4, 1)  (5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Corpus contains pairs - a unique id for each word in the document and its frequency (word_id, word_frequency).\n",
    "print(pd.DataFrame(corpus[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
      " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]]\n",
      "rutte\n"
     ]
    }
   ],
   "source": [
    "# We also can check what was the word from the dictionary - id2word\n",
    "pprint(corpus[:2])\n",
    "print(id2word[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('droogkloot_marcel', 1),\n",
       "  ('helemaal', 1),\n",
       "  ('moeder_luister', 1),\n",
       "  ('nederlands_bekendste', 1),\n",
       "  ('roosmalen_sloopt', 1),\n",
       "  ('topper', 1)],\n",
       " [('alleen_ollongren', 1),\n",
       "  ('benul', 1),\n",
       "  ('kennelijk_enig', 1),\n",
       "  ('minister', 1),\n",
       "  ('minister_ee', 1),\n",
       "  ('rutte', 1),\n",
       "  ('staatsrecht', 1)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Readable format\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: tune the parameters\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, # corpus\n",
    "                                           id2word=id2word, # dict\n",
    "                                           num_topics=10, # number of topic to be extracted\n",
    "                                           random_state=100,\n",
    "                                           update_every=1, #\n",
    "                                           chunksize=100, # \n",
    "                                           passes=10, # \n",
    "                                           alpha='auto', # \n",
    "                                           per_word_topics=True) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Print the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.123*\"racisme\" + 0.105*\"baudet\" + 0.075*\"fvd\" + 0.073*\"doet\" + '\n",
      "  '0.033*\"hart\" + 0.029*\"aantijgingen\" + 0.029*\"idiote\" + '\n",
      "  '0.029*\"wervelende_week\" + 0.029*\"vol_onterechte\" + 0.021*\"opvalt\"'),\n",
      " (1,\n",
      "  '0.027*\"partijkartel\" + 0.025*\"steeds\" + 0.020*\"jaar\" + 0.019*\"helemaal\" + '\n",
      "  '0.015*\"groningen\" + 0.012*\"zon\" + 0.011*\"nederland\" + 0.011*\"politiek\" + '\n",
      "  '0.011*\"motie\" + 0.010*\"zullen\"'),\n",
      " (2,\n",
      "  '0.062*\"leefbaar\" + 0.052*\"rotterdam\" + 0.041*\"co\" + 0.037*\"regering\" + '\n",
      "  '0.027*\"boom\" + 0.026*\"joost_eerdmans\" + 0.022*\"sluit\" + 0.016*\"nleugens\" + '\n",
      "  '0.010*\"zelfs\" + 0.010*\"vast\"'),\n",
      " (3,\n",
      "  '0.082*\"referendum\" + 0.060*\"weer\" + 0.054*\"gaat\" + 0.041*\"schaffen\" + '\n",
      "  '0.041*\"raadgevend\" + 0.040*\"proberen\" + 0.040*\"komende_donderdag\" + '\n",
      "  '0.040*\"tegenover\" + 0.031*\"co\" + 0.027*\"gaan\"'),\n",
      " (4,\n",
      "  '0.104*\"pechtold\" + 0.097*\"media\" + 0.061*\"kabinet\" + 0.058*\"discriminatie\" '\n",
      "  '+ 0.026*\"politicus\" + 0.021*\"ras\" + 0.012*\"ooit\" + 0.011*\"geschapen\" + '\n",
      "  '0.007*\"geven\" + 0.006*\"alexander\"'),\n",
      " (5,\n",
      "  '0.076*\"pvda\" + 0.065*\"donorwet\" + 0.045*\"yernaz_ramautarsing\" + '\n",
      "  '0.020*\"komen_hopelijk\" + 0.020*\"zieltogende\" + 0.019*\"weg_terug\" + '\n",
      "  '0.019*\"nederland_afgeschreven\" + 0.016*\"interview\" + 0.015*\"debatteren\" + '\n",
      "  '0.014*\"senaat\"'),\n",
      " (6,\n",
      "  '0.099*\"co\" + 0.078*\"gemeente\" + 0.067*\"mag\" + 0.058*\"nl\" + 0.058*\"af\" + '\n",
      "  '0.055*\"hallo\" + 0.055*\"detailhandel_werk\" + 0.055*\"veil\" + 0.017*\"waar\" + '\n",
      "  '0.016*\"groen\"'),\n",
      " (7,\n",
      "  '0.037*\"vvd\" + 0.037*\"handig\" + 0.031*\"cda\" + 0.030*\"partij\" + '\n",
      "  '0.027*\"partijen\" + 0.021*\"rt\" + 0.019*\"neemt\" + 0.019*\"vormen\" + '\n",
      "  '0.017*\"weg\" + 0.017*\"maakt\"'),\n",
      " (8,\n",
      "  '0.109*\"gewoon\" + 0.102*\"uitspraken\" + 0.068*\"rutte\" + 0.026*\"beloven\" + '\n",
      "  '0.026*\"verkiezingstijd_neerst\" + 0.026*\"opnemen\" + 0.026*\"armoe\" + '\n",
      "  '0.014*\"geeft\" + 0.013*\"stemmen\" + 0.011*\"minister\"'),\n",
      " (9,\n",
      "  '0.096*\"mee\" + 0.067*\"keer\" + 0.053*\"vind\" + 0.051*\"zorgelijk\" + '\n",
      "  '0.031*\"feiten_verdraait\" + 0.026*\"burgers\" + 0.021*\"begint_gevaarlijke\" + '\n",
      "  '0.021*\"nemen_htt\" + 0.020*\"zorg\" + 0.019*\"beter\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the keywords in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.294330783861692\n",
      "\n",
      "Coherence Score:  0.5200181958046101\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bubble is a topic. The larger the bubble, the more prevalent the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikhail\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Mallet Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
